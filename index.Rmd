---
title: "Data Science Class 8 Project"
author: "Blake Anderson"
date: "Thursday, January 22, 2015"
output: html_document
---


Question: Create a model to accurately determine if the lift was performed 
correctly using the given data set.

Libraries to add
```{r,echo=TRUE}
        library(caret)
        library(randomForest)
        set.seed(115)
```


Import the data.
```{r,echo=TRUE}
### Download the databases
        setwd("c:/R")
#        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv","c:/R/train.csv")
#        download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv","c:/R/test.csv")
###      download.file are includes as notes, so I am not redownloading data
###      from someone who was kind enough share their data with this large class.
### Loading the variables 
        test<-read.csv("./test.csv")
        train<-read.csv("./train.csv")
```





The dataset included 160 variables. The variables were inspected. The variable 
X is the index of the sample, and the sample data is sorted by results variable 
"classe". The timestamp variables were also removed.  It is possible that the
subject could have been asked to do exercises correctly and then incorrectly.  
Therefore, the sample structure could be a spurious correlation.  The classe variable
was transfered to a response variable. Also, columns with Nas in the test variable 
were removed from both the train and test dataset.  The additional varaibles were
removed in order to cut down on calculation time.

```{r,echo=TRUE}
# Removing column ID and time data 
        train_1<-train[,!(names(train) %in% c("X","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp"))]
        test_1<-test[,!(names(train) %in% c("X","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp"))]
# Output Variable
        response<-train$classe
# Remove any columns with NAs
        train_2<-train_1[ , colSums(is.na(test_1)) == 0]
        test_2<-test_1[ , colSums(is.na(test_1))==0]
```
The observation variables have different data classes.  Several of the factor 
variables have more than 20 factors, which causes error with some machine
learning algorithims.  The train and test data had all factor variables converted
to character variables then numeric variables.  The output variable "classe" was 
added back into the train dataset, because machine learning algorithims perform
better when the output variable is a factor variable.

```{r,echo=TRUE}
### Changing the factor variables into numeric variables
        ids <- sapply(train_2, is.factor)
        ids_t <- sapply(test_2, is.factor)
        train_3=train_2
        test_3=test_2
        train_3[ids] <- lapply(train_2[ids], as.character)
        test_3[ids_t] <- lapply(test_2[ids_t], as.character)
        train_3[ids] <- lapply(train_2[ids], as.numeric)
        test_3[ids_t] <- lapply(test_2[ids_t], as.numeric)

## Adding back in the factor Variable
        train_3[,56]<-train[,160]
```
The dataset has 19622 observations/samples. There are enough observations to do 
cross-validation. The test data has only 20 observations, so this is not sufficient 
to determine the model accuracy.  70% of the training data set was set aside
for testing different models (machine learning algorithims). The remaining 30% 
of the training data set was divided probe and quiz datasets.  The probe dataset
was used to test each model developed. The model with the highest accuracy 
was selected. The quiz dataset (errot_test) was used to determine 
the accuracy of the model selected. A model was only to be applied once to the 
quiz (error test) data.  

```{r,echo=TRUE}      
        inTrain_1 <- createDataPartition(y=train_3$classe,
                              p=0.7, list=FALSE)
                Multi_Testing <- train_3[inTrain_1,]
                test_Models <- train_3[-inTrain_1,]
     
        inTrain_2 <- createDataPartition(y=test_Models$classe,
                              p=0.7, list=FALSE)
                Model_Testing <- test_Models[inTrain_2,]
                 error_test <- test_Models[-inTrain_2,]

### removing the output data
        Y_Multi_Testing<-Multi_Testing[,56]
        Multi_Testing<-Multi_Testing[,1:55]
```
In the random forest lecture, "random forests were usually one of the top two
performing algorithims in prediciton contests." Therefore, the  random forest 
algorithm was selected to create the first model.  

```{r,echo=TRUE}
        model_rf<-randomForest(x=Multi_Testing,y=Y_Multi_Testing)
```
The random forest model was used to predict the classe variable in the training 
dataset. 

```{r,echo=TRUE}
### adding the classe data back to the Multi_testing Dataset
        Multi_Testing<-cbind(Multi_Testing,Y_Multi_Testing)
        output_rf<-predict(model_rf,Multi_Testing)
        Multi_Testing$predRight<-output_rf==Multi_Testing$Y_Multi_Testing
        table(output_rf,Multi_Testing$Y_Multi_Testing)
```

Then the accuracy of the model on the training dataset was determined.
```{r,echo=TRUE}
        
        conf_RF_train<-confusionMatrix(output_rf,Multi_Testing$Y_Multi_Testing)
        accuracy_train<-conf_RF_train[[3]][1]
        accuracy_train
```
The developed random forrest model perfectly predicts the training dataset, so
the in sample error rate is 0% on 13737. This accuracy suggests that the model 
could be overfitting. 


The random forest model was used to predict the classe variable in the model
testing dataset. 
```{r,echo=TRUE}
        output_rf<-predict(model_rf,Model_Testing)
        Model_Testing$predRight<-output_rf==Model_Testing$classe
```

The model prediction table is shown below.
```{r,echo=TRUE}
        table(output_rf,Model_Testing$classe)
```

The model accurary was taken from a confusion matrix. 
```{r,echo=TRUE}
        conf_RF<-confusionMatrix(output_rf,Model_Testing$classe)
        accuracy<-conf_RF[[3]][1]
        accuracy
```
Out of Sample Error

The accuracy of the first model tested on the model testing dataset was 99.6%.  
The first model tested has a very high accuracy with the model dataset.  Since
the accuracy on the training dataset and the model testing database are above
99%, the model is not overfitting significantly. Since accuracy and not 
computation time was the goal of this assignment, no additional models were 
developed.  Also, only one model has been applied to the model testing dataset.
Therefore, the results from applying the model to the model test dataset can 
be assumed to be the accuracy of the model.  The model accuracy was found to 
be 99.6%.  Since the model testing data was not used to develop the model, 
a valid estimate of the out of sample error is the error rate of 0.4% from 
the model testing data.


The final model was applied to the given 20 observation test dataset. The 
results were fed into the machine learning website. The model perfectly 
predicted the 20 test observations confirming the validity of the model.
```{r,echo=TRUE}
output_test<-predict(model_rf,Model_Testing)
```
